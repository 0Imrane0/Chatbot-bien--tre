# üöÄ Guide d'Entra√Ænement GPU - Approche 3

## üéØ Objectif

Entra√Æner le mod√®le BERT fine-tun√© sur GPU gratuit (Google Colab) pour l'Approche 3.

**Dur√©e totale : 5-10 minutes** (dont 2-3 min d'entra√Ænement GPU)

---

## üìã Option 1 : Google Colab (RECOMMAND√â ‚≠ê)

### √âtape 1 : Ouvrir le notebook

1. Ouvrir Google Colab : https://colab.research.google.com/
2. `File` ‚Üí `Upload notebook`
3. S√©lectionner : `notebooks/02_finetuning_bert_gpu.ipynb`

### √âtape 2 : Activer le GPU

**‚ö° CRITIQUE - Sans GPU, √ßa prend 10 minutes!**

1. Menu : `Runtime` ‚Üí `Change runtime type`
2. `Hardware accelerator` ‚Üí S√©lectionner **T4 GPU**
3. Cliquer `Save`

### √âtape 3 : Ex√©cuter toutes les cellules

1. Menu : `Runtime` ‚Üí `Run all`
2. Attendre 2-3 minutes ‚è≥
3. Le mod√®le s'entra√Æne automatiquement

### √âtape 4 : T√©l√©charger le mod√®le

**Option A : T√©l√©chargement direct**
```python
# Ex√©cuter la cellule de compression
!zip -r bert_finetuned_final.zip bert_finetuned_final/
```
- Clic droit sur `bert_finetuned_final.zip` dans l'explorateur
- T√©l√©charger (~280 MB)

**Option B : Sauvegarder sur Google Drive**
```python
from google.colab import drive
drive.mount('/content/drive')
!cp -r bert_finetuned_final /content/drive/MyDrive/
```

### √âtape 5 : Installer localement

1. Extraire `bert_finetuned_final.zip`
2. Copier le dossier dans : `models/approach3/bert_finetuned/`
3. Structure finale :
```
models/
  approach3/
    bert_finetuned/
      config.json
      pytorch_model.bin
      tokenizer_config.json
      vocab.txt
      special_tokens_map.json
```

### √âtape 6 : Tester

```bash
cd "C:\Users\LOQ\Documents\Chatbot bien-√™tre"
python compare_approaches.py
```

‚úÖ Vous devriez voir la comparaison Approche 1 vs Approche 3!

---

## üìã Option 2 : Kaggle (Alternative)

### Avantages Kaggle
- 30h/semaine de GPU gratuit (vs 12h/jour Colab)
- GPU T4 ou P100
- Pas de d√©connexion automatique

### √âtapes

1. Cr√©er compte : https://www.kaggle.com/
2. `Notebooks` ‚Üí `New Notebook`
3. Settings ‚Üí `Accelerator` ‚Üí **GPU T4**
4. Copier-coller le code du notebook Colab
5. Run all
6. Download output

---

## üîç V√©rification du GPU

Avant d'entra√Æner, v√©rifier que le GPU est actif :

```python
import torch

if torch.cuda.is_available():
    print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")
    print(f"   M√©moire: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
else:
    print("‚ùå GPU non disponible!")
```

**R√©sultat attendu :**
```
‚úÖ GPU: Tesla T4
   M√©moire: 15.0 GB
```

---

## üìä Performances attendues

| Configuration | Temps entra√Ænement (3 epochs) | Pr√©cision finale |
|---------------|-------------------------------|------------------|
| **GPU T4** (Colab) | ‚ö° 2-3 minutes | ~91-95% |
| **GPU P100** (Kaggle) | ‚ö° 1-2 minutes | ~91-95% |
| CPU (local) | üê¢ 10-15 minutes | ~91-95% |

---

## üéØ R√©sum√© de l'entra√Ænement

### Donn√©es
- **500 exemples** bien-√™tre
- **5 classes** : tr√®s n√©gatif, n√©gatif, neutre, positif, tr√®s positif
- **100 exemples par classe** (√©quilibr√©)
- **Split 80/20** : 400 train / 100 validation

### Hyperparam√®tres
- **Learning rate** : 2e-5 (standard pour fine-tuning)
- **Batch size** : 16 (GPU) ou 8 (CPU)
- **Epochs** : 3
- **Optimizer** : AdamW
- **Weight decay** : 0.01

### R√©sultats attendus

**Training loss** :
- Epoch 1 : ~1.2
- Epoch 2 : ~0.6
- Epoch 3 : ~0.4

**Validation loss** :
- Epoch 1 : ~0.8
- Epoch 2 : ~0.5
- Epoch 3 : ~0.4

**Pr√©cision validation** : ~91-95%

---

## ‚ùì Troubleshooting

### Probl√®me 1 : GPU non disponible sur Colab

**Solution :**
1. Runtime ‚Üí Disconnect and delete runtime
2. Runtime ‚Üí Change runtime type ‚Üí T4 GPU
3. Runtime ‚Üí Run all

### Probl√®me 2 : Out of memory (OOM)

**Solution :** R√©duire le batch size
```python
per_device_train_batch_size=8,  # Au lieu de 16
```

### Probl√®me 3 : T√©l√©chargement lent

**Solution :** Utiliser Google Drive
```python
from google.colab import drive
drive.mount('/content/drive')
!cp -r bert_finetuned_final /content/drive/MyDrive/
```

### Probl√®me 4 : Mod√®le ne charge pas localement

**V√©rifier la structure :**
```bash
ls models/approach3/bert_finetuned/
```

**Fichiers requis :**
- config.json
- pytorch_model.bin
- tokenizer_config.json
- vocab.txt

---

## üöÄ Commandes rapides

### Entra√Ænement local (CPU - si vous insistez)
```bash
cd "C:\Users\LOQ\Documents\Chatbot bien-√™tre"
python src/approach3/train_finetuner.py
```
‚è≥ Dur√©e : 10-15 minutes

### Comparaison Approche 1 vs 3
```bash
python compare_approaches.py
```

### Test du chatbot Approche 3
```bash
python -c "from src.approach3.chatbot import WellbeingChatbot; bot = WellbeingChatbot(); bot.start_conversation()"
```

---

## ‚úÖ Checklist finale

- [ ] Notebook upload√© sur Colab
- [ ] GPU T4 activ√©
- [ ] Toutes les cellules ex√©cut√©es
- [ ] Mod√®le t√©l√©charg√© (`bert_finetuned_final.zip`)
- [ ] Mod√®le extrait dans `models/approach3/bert_finetuned/`
- [ ] Test√© avec `compare_approaches.py`
- [ ] Approche 3 fonctionne! üéâ

---

## üìö Ressources

- **Google Colab** : https://colab.research.google.com/
- **Kaggle Notebooks** : https://www.kaggle.com/code
- **HuggingFace Docs** : https://huggingface.co/docs/transformers/
- **PyTorch GPU** : https://pytorch.org/get-started/locally/

---

**Bon entra√Ænement! üöÄ**
